// constants.js
export const ABSTRACT = `Egocentric vision perception of first-person videos captured
            by a wearable cameras has gained popularity in social
            robotics and XR applications, as the methodology affords opportunity to infer user actions within the user’s naturalistic
            environment and perspective. Reconstructing a human user’s
            body motion and actions from first-person camera videos
            presents significant challenges due to self-occlusions of body
            parts seamlessly moving in and out of the frame. While extensive research leveraging pose estimation techniques have
            overcome many of these constraints, similar advances using
            3D mesh recovery of first-person camera video have not been
            met. In this paper, we propose an fish-eye aware transformerbased model, called Fish2Mesh [F2M] Transformer using
            multi-task heads for SMPL parametric-regression and camera transformations, specifically designed for egobody human mesh reconstruction. We utilize the pre-trained model,
            4D-Human to create a training dataset by setting up 3rd person cameras for weak supervisions, to compensate for the
            lack of egocentric camera data. The experiments demonstrate
            our model’s superior 3D human mesh reconstruction performance, surpassing the previous state-of-the-art models on robustness and reliability. We will publish our codes and dataset
            on the public links.`

export const OVERVIEW = "Overview of the project";
